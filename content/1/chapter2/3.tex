
收集关于程序性能信息的最简单的方法是运行，并测量运行所花费的时间。当然，要进行有效的优化，我们需要更多的数据：最好知道程序的哪些部分耗时最长，这样我们就不会浪费自己的时间来优化代码，这些代码可能非常低效，但也只需要很少的时间，因此不会对最终结果有任何贡献。

我们在示例程序中添加计时器时，已经看到了一个简单的例子：现在我们知道排序本身需要多长时间。简而言之，这就是基准测试的全部。其余的工作都是体力活，用计时器检测代码，收集信息，并以可读的格式进行报告。看看有什么工具可以实现这一点，先从语言本身提供的计时器开始。

\subsubsubsection{2.3.1\hspace{0.2cm}C++的chrono计时器}

C++在它的chrono库中有一些工具可以用来收集计时信息。可以测量程序中通过任意两点之间所需的时间:

%\hspace*{\fill} \\ %插入空行
\noindent
\textbf{example.C}
\begin{lstlisting}[style=styleCXX]
#include <chrono>
using std::chrono::duration_cast;
using std::chrono::milliseconds;
using std::chrono::system_clock;
  …
auto t0 = system_clock::now();
  … do some work …
auto t1 = system_clock::now();
auto delta_t = duration_cast<milliseconds>(t1 – t0);
cout << "Time: " << delta_t.count() << endl;
\end{lstlisting}

应该指出的是，C++计时时钟测量的是实时时间(通常称为挂钟时间)。通常，这就是想要测量的时间。更详细的分析通常需要测量CPU时间，即CPU的工作时，以及在CPU空闲时的时间。单线程程序中，CPU时间不能大于实际时间，当程序是计算密集型时，这两个时间理论上相同，这意味着完全使用CPU进行计算了。另一方面，用户界面程序会把大部分时间都花在等待用户和闲置CPU上，所以我们希望CPU时间尽可能的短：这样的程序是高效的，并且使用尽可能少的CPU资源来满足用户的请求。因此，我们必须超越C++17。


\subsubsubsection{2.3.2\hspace{0.2cm}高精度计时器}

为了测试CPU时间，我们必须使用特定于操作系统的系统函数，在Linux和其他posix兼容的系统上，可以使用\texttt{clock\_gettime()}来使用硬件的高精度计时器:

%\hspace*{\fill} \\ %插入空行
\noindent
\textbf{clocks.C}
\begin{lstlisting}[style=styleCXX]
timespec t0, t1;
clockid_t clock_id = …; // Specific clock
clock_gettime(clock_id, &t0);
  … do some work …
clock_gettime(clock_id, &t1);
double delta_t = t1.tv_sec – t0.tv_sec +
  1e-9*(t1.tv_nsec – t0.tv_nsec);
\end{lstlisting}

第二个参数的函数返回当前时间，\texttt{tv\_sec}是过去某一时刻到现在的秒数，\texttt{tv\_nsec}是上一整个秒到现在的纳秒数。时间的起源并不重要，因为我们总是测量时间间隔。这里要先减去秒，然后再加纳秒。

我们可以在前面的代码中使用的几个硬件计时器，其中一个由\texttt{clock\_id}变量的值选择，这些计时器中的计时器与我们已经使用过的系统或实时时钟相同。ID为\texttt{CLOCK\_REALTIME}。我们感兴趣的另外两个计时器是两个CPU计时器:\texttt{CLOCK\_PROCESS\_CPUTIME\_ID}是测量当前程序使用的CPU时间的计时器，而\texttt{CLOCK\_THREAD\_CPUTIME\_ID}是测量调用线程使用的时间的计时器。

对代码进行基准测试时，从多个计时器的测量结果通常是很有帮助的。一个单线程程序不间断进行计算时，三个计时器应该返回相同的结果:

%\hspace*{\fill} \\ %插入空行
\noindent
\textbf{clocks.C}
\begin{lstlisting}[style=styleCXX]
double duration(timespec a, timespec b) {
	return a.tv_sec - b.tv_sec + 1e-9*(a.tv_nsec - b.tv_nsec);
}
…
{
	timespec rt0, ct0, tt0;
	clock_gettime(CLOCK_REALTIME, &rt0);
	clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ct0);
	clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tt0);
	constexpr double X = 1e6;
	double s = 0;
	for (double x = 0; x < X; x += 0.1) s += sin(x);
	timespec rt1, ct1, tt1;
	clock_gettime(CLOCK_REALTIME, &rt1);
	clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ct1);
	clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tt1);
	cout << "Real time: " << duration(rt1, rt0) << "s, "
	  "CPU time: " << duration(ct1, ct0) << "s, "
	  "Thread time: " << duration(tt1, tt0) << "s" <<
	  endl;
}
\end{lstlisting}

Here the "CPU-intensive work" is some kind of computation, and all three times should be almost identical. You can observe this in a simple experiment with any kind of computation. The values of the times will depend on the speed of the computer, but, that aside, the result should look like this:

\begin{tcblisting}{commandshell={}}
Real time: 0.3717s, CPU time: 0.3716s, Thread time: 0.3716s
\end{tcblisting}

If the reported CPU time does not match the real time, it is likely that the machine is overloaded (many other processes are competing for the CPU resources), or the program is running out of memory (if the program uses more memory than the physical memory on the machine, it will have to use the much slower disk swap, and the CPUs can't do any work while the program is waiting for the memory to be paged in from disk).

On the other hand, if the program does not compute much but instead, waits on user input, or receives the data from the network, or does some other work that does not take many CPU resources, we will see a very different result. The simplest way to observe this behavior is by calling the sleep() function instead of the computation we used earlier:

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{clocks.C}
\begin{lstlisting}[style=styleCXX]
{
	timespec rt0, ct0, tt0;
	clock_gettime(CLOCK_REALTIME, &rt0);
	clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ct0);
	clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tt0);
	sleep(1);
	timespec rt1, ct1, tt1;
	clock_gettime(CLOCK_REALTIME, &rt1);
	clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ct1);
	clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tt1);
	cout << "Real time: " << duration(rt1, rt0) << "s, "
	  "CPU time: " << duration(ct1, ct0) << "s, "
	  "Thread time: " << duration(tt1, tt0) << "s" <<
  	  endl;
}
\end{lstlisting}

Now we will, hopefully, see that a sleeping program uses very little CPU:

\begin{tcblisting}{commandshell={}}
Real time: 1.000s, CPU time: 3.23e-05s, Thread time: 3.32e-05s
\end{tcblisting}

The same should be true for a program that is blocked on a socket or a file or is waiting for a user action.

So far, we have not seen any difference between the two CPU timers, and you will not see any unless your program uses threads. We can make our compute-heavy program do the same work but use a separate thread for it:

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{clocks.C}
\begin{lstlisting}[style=styleCXX]
{
	timespec rt0, ct0, tt0;
	clock_gettime(CLOCK_REALTIME, &rt0);
	clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ct0);
	clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tt0);
	constexpr double X = 1e6;
	double s = 0;
	auto f = std::async(std::launch::async,
	  [&]{ for (double x = 0; x < X; x += 0.1) s += sin(x);
	  });
	f.wait();
	timespec rt1, ct1, tt1;
	clock_gettime(CLOCK_REALTIME, &rt1);
	clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ct1);
	clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tt1);
	cout << "Real time: " << duration(rt1, rt0) << "s, "
	  "CPU time: " << duration(ct1, ct0) << "s, "
	  "Thread time: " << duration(tt1, tt0) << "s" <<
	  endl;
}
\end{lstlisting}

The total amount of computations remains the same, and there is still only one thread doing the work, so we do not expect any changes to the real time or the process-wide CPU time. However, the thread that is calling the timers is now idle; all it does is wait on the future returned by std::async until the work is done. This waiting is very similar to the sleep() function in the previous example, and we can see it in the results:

\begin{tcblisting}{commandshell={}}
Real time: 0.3774s, CPU time: 0.377s, Thread time: 7.77e-05s
\end{tcblisting}

Now the real time and the process-wide CPU time look like those from the "heavy computing" example, but the thread-specific CPU time is low, like in the "sleeping" example. That is because the overall program is doing heavy computing, but the thread that calls the timers is indeed mostly sleeping.

Most of the time, if we are going to use threads for computing, the goal is to do more computations faster, so we will use several threads and spread the work between them. Let us modify the preceding example to compute also on the main thread:

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{clocks.C}
\begin{lstlisting}[style=styleCXX]
{
	timespec rt0, ct0, tt0;
	clock_gettime(CLOCK_REALTIME, &rt0);
	clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ct0);
	clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tt0);
	constexpr double X = 1e6;
	double s1 = 0, s2 = 0;
	auto f = std::async(std::launch::async,
	  [&]{ for (double x = 0; x < X; x += 0.1) s1 += sin(x);
	  });
	for (double x = 0; x < X; x += 0.1) s2 += sin(x);
	f.wait();
	timespec rt1, ct1, tt1;
	clock_gettime(CLOCK_REALTIME, &rt1);
	clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ct1);
	clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tt1);
	cout << "Real time: " << duration(rt1, rt0) << "s, "
	  "CPU time: " << duration(ct1, ct0) << "s, "
	  "Thread time: " << duration(tt1, tt0) << "s" <<
	  endl;
}
\end{lstlisting}

Now both threads are doing computations, so the CPU time used by the program passes at a double rate compared to the real time:

\begin{tcblisting}{commandshell={}}
Real time: 0.5327s, CPU time: 1.01s, Thread time: 0.5092s
\end{tcblisting}

This is pretty good: we have done 1 second worth of computations in only 0.53 seconds of real time. Ideally, this would have been 0.5 seconds, but in reality, there is some overhead for launching threads and waiting for them. Also, one of the two threads might have taken slightly longer to do the work, then the other thread was idle some of the time.

Benchmarking a program is a powerful way to collect performance data. Simply by observing the time it takes to execute a function or handle an event, we can learn a lot about the performance of the code. For compute-intensive code, we can see whether the program is indeed doing computations non-stop or is waiting on something. For multi-threaded programs, we can measure how effective the concurrency is and what the overhead is. But we are not just limited to collecting execution times: we can also report any counts and values we deem relevant: how many times a function was called, how long the average string we sort is, anything we need to help us interpret the measurements.

However, this flexibility comes at a price: with benchmarking, we can answer almost any question about the performance of the program that we want to ask. But we have to ask the question first: we report only what we decided to measure. If we want to know how long a certain function takes, we have to add the timers to it; if they aren't there, we will learn nothing until we rewrite the code and rerun the benchmark. On the other hand, it would not do to sprinkle timers everywhere in the code: these function calls are fairly expensive, so using too many can both slow down your program and distort the performance measurements. With experience and good coding discipline, you can learn to instrument the code you write in advance, so at least its major sections can be benchmarked easily.

But what should you do if you have no idea where to start? What if you have inherited a code base that was not instrumented for any benchmarking? Or, maybe, you isolated your performance bottleneck to a large section of code, but there are no more timers  inside of it? One approach is to continue instrumenting the code until you have enough data to analyze the problem. But this brute-force approach is slow, so you will want some guidance on where to focus your efforts. This is where profiling comes in: it lets you collect performance data for a program that wasn't instrumented, by hand, for easy benchmarking. We will learn about profiling in the next section.






