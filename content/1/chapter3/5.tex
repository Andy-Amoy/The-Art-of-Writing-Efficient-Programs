现在，我们对处理器高效使用的理解是:首先，CPU可以同时做多个操作，比如：同时做加法和乘法。不利用这种能力就是暴殄天物。此外，限制实现效率最大化能力的因素是，能够以多快的速度生成数据，以提供给这些操作。受到数据依赖关系的约束：如果一个操作计算了下一个操作用作输入的值，那么这两个操作必须顺序执行。处理这种依赖关系的方法是流水线操作:当执行循环或长的代码序列时，处理器将交叉计算，如循环迭代，只要它们有可以独立执行的操作。

流水线也使用的前提条件。流水线的\textbf{提前计划}:为了从循环迭代中交叉执行代码，必须知道将执行什么代码。将这与上一节中了解到的进行比较:为了并行执行指令，必须预先知道输入值是什么。现在，为了在流水线中运行指令，必须知道指令是什么。我们知道吗？因为运行的代码通常依赖于数据，每次遇到\texttt{if(条件)}语句时，要么执行\texttt{true}分支，要么执行\texttt{false}分支，但是在确定\textit{条件}之前我们并不知道会执行到哪个分支。像数据依赖是指令级并行的障碍一样，条件执行或分支也是流水线的障碍。

随着流水线的中断，我们可以预期程序的效率会显著降低。我们可以很容易地修改之前的基准测试来观察条件的这种有害影响，例如不要写这样的代码:

\begin{lstlisting}[style=styleCXX]
a1 += p1[i] + p2[i];
\end{lstlisting}

可以这样写:

\begin{lstlisting}[style=styleCXX]
a1 += (p1[i]>p2[i]) ? p1[i] : p2[i];
\end{lstlisting}

现在我们将数据依赖重新引入到代码中了:

%\hspace*{\fill} \\ %插入空行
\begin{center}
\includegraphics[width=0.6\textwidth]{content/1/chapter3/images/18.jpg}\\
图3.18 - 转移指令对流水线的影响
\end{center}

没有好方法将这段代码转换为要执行的线性指令流，并且需要处理不能避免条件跳转。

实际情况要复杂一些:我们的基准测试可能会出现性能的显著下降，也可能不会。原因是许多处理器都有某种\textbf{条件移动}，甚至是\textbf{条件添加}指令，编译器可能决定使用它们。如果发生这种情况，我们的代码就会变得完全有序，没有跳转或分支，并且可以完美地流水线化:

%\hspace*{\fill} \\ %插入空行
\begin{center}
\includegraphics[width=0.9\textwidth]{content/1/chapter3/images/19.jpg}\\
图3.19 - cmove将分支流水线化
\end{center}

x86的CPU有一个条件移动指令\texttt{cmove}(虽然不是所有编译器都使用它来实现\texttt{?:}操作符)。具有AVX或AVX2指令集的处理器具有一组强大的掩码加法和乘法指令，这些指令也可以用于实现一些条件代码。这就是为什么在用分支对代码进行基准测试和优化时，检查生成的目标代码并确认代码中确实包含分支，以及确定它们是否影响了性能。还有一些分析器工具可以用于此目的，我们稍后介绍。

虽然分支和条件在大多数现实程序中无处不在，但当程序减少到只有几行代码时，它们就会消失，原因是编译器可能使用前面提到的条件指令。构造糟糕的基准测试中的另一个原因是，编译器能够在编译时计算出条件的值，例如：大多数编译器将完全优化代码，比如：\texttt{if (true)}或\texttt{if (false)}生成的代码中就没有这个语句的痕迹，任何永远不会执行的代码也会消除。要了解分支对循环流水线的有害影响，我们必须构造一个编译器无法预测条件检查结果的测试，可以从实际使用的程序中提取了一个数据集。下一个演示，我们将使用随机值:

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{02\_branch.C}
\begin{lstlisting}[style=styleCXX]
std::vector<unsigned long> v1(N), v2(N);
std::vector<int> c1(N);
for (size_t i = 0; i < N; ++i) {
	v1[i] = rand();
	v2[i] = rand();
	c1[i] = rand() & 1;
}
unsigned long* p1 = v1.data();
unsigned long* p2 = v2.data();
int* b1 = c1.data();
for (auto _ : state) {
	unsigned long a1 = 0, a2 = 0;
	for (size_t i = 0; i < N; ++i) {
		if (b1[i]) {
			a1 += p1[i];
		} else {
			a1 *= p2[i];
		}
	}
	benchmark::DoNotOptimize(a1);
	benchmark::DoNotOptimize(a2);
	benchmark::ClobberMemory();
}
\end{lstlisting}

Again, we have two input vectors v1 and v2, plus a control vector c1 that has random values of zero and one (avoid using vector<bool> here, it is not an array of bytes but a packed array of bits, so accessing it is considerably more expensive, and we are not interested in benchmarking bit manipulation instructions at this time). The compiler cannot predict whether the next random number is odd or even, thus, no optimizations are possible. Also, we have examined the generated machine code and confirmed that our compiler (Clang-11 on x86) implements this loop using a simple conditional jump. To have a baseline, we will compare the performance of this loop with one that does unconditional addition and multiplication on each iteration: a1 += p1[i]*p2[i]. This simpler loop does both an addition and a multiplication on each iteration; however, thanks to the pipelining, we get the addition free: it is executed simultaneously with the multiplication from the next iteration. The conditional branch, on the other hand, is anything but free:

%\hspace*{\fill} \\ %插入空行
\begin{center}
\includegraphics[width=0.9\textwidth]{content/1/chapter3/images/20.jpg}\\
图 3.20
\end{center}

As you can see, the conditional code is about five times slower than the sequential one. This confirms our prediction that when the next instruction depends on the result of the previous one, the code cannot be effectively pipelined.


\subsubsubsection{3.5.1\hspace{0.2cm}分支预测}

However, an astute reader may point out that the picture we have just described cannot possibly be complete, or even true: let us go back, for a moment, to the apparently linear code such as the loop we have used extensively in the last section:

\begin{lstlisting}[style=styleCXX]
	for (size_t i = 0; i < N; ++i) {
		a1 += v1[i] + v2[i]; // s[i] = v1[i] + v2[i]
	}
\end{lstlisting}

Here is what the body of this loop looks like from the processor's point of view:

\hspace*{\fill} \\ %插入空行
\begin{center}
\includegraphics[width=0.9\textwidth]{content/1/chapter3/images/21.jpg}\\
Figure 3.21 – Loop executed in a pipeline of width w
\end{center}

In Figure 3.21, we have shown three interleaved iterations, but there could be even more, the total width of the pipeline is w, and ideally, w is large enough that at every cycle, the CPU is executing exactly as many instructions as it can execute simultaneously (such peak efficiency is rarely possible in practice). Note, however, that it may be impossible to access v[i+2] at the same time as we compute the sum p1[i] + p2[i]: there is no guarantee that the loop has two more iterations to go, and, if it doesn't, the element v[i+2] does not exist and accessing it results in undefined behavior. There is a hidden conditional in the previous code: at every iteration, we must check if i is less than N, and only then can we execute the instructions of the i-th iteration. 

Therefore, our comparison in Figure 3.20 is a lie: we did not compare pipelined sequential execution versus an unpredictable conditional one. Both benchmarks are, in fact, examples of conditional code, they both have branches.

The full truth is somewhere in between. To understand it, we have to learn about the antidote to the conditional execution, which poisons the pipelining and is itself the antidote to the data dependency. The way to save the pipelining in the presence of branches is to attempt to convert the conditional code to the sequential one. Such conversion could be done if we knew in advance which path the branch is going to take: we would simply eliminate the branch and proceed to the next instruction to be executed. Of course, there would be no need even to write such code if we knew in advance what the condition is. Still, consider the loop termination condition. Assuming the loop is executed many times, it is a good bet that the condition i < N evaluates to true (we would lose this bet only one out of N times).

The processor makes the same bet using the technique known as branch prediction. It analyzes the history of every branch in the code and assumes that the behavior will not change in the future. For the end of the loop condition, the processor will quickly learn that most of the time, it has to proceed to the next iteration. Therefore, the right thing to do is to pipeline the next iteration as if we are sure it's going to happen. Of course, we have to defer the actual writing of the results into memory until we evaluate the condition and confirm that the iteration does happen; the processor has a certain number of write buffers to hold such unconfirmed results in limbo before committing them to memory.

The pipeline for the loop with just an addition, therefore, does look exactly as shown in Figure 3.21. The only catch is that, when starting to execute iteration i+2 before the i-th iteration is complete, the processor is making a bet based on its prediction of whether the conditional branch will be taken or not. Such execution of the code before we know for sure that this code really exists is known as speculative execution. If the bet is won, we already have the results by the time we figure out that we needed the computation, and all is well. If the processor loses the bet, it has to discard some of the computations to avoid producing incorrect results: for example, writing into memory overwrites what was there before and cannot be undone on most hardware platforms, while computing the result and storing it in a register is entirely reversible, except for the time we wasted, of course.

We now have a more complete picture of how the pipelining really works: in order to find more instructions to execute in parallel, the processor looks at the code for the next iterations of the loop and starts to execute it simultaneously with the current iteration. If the code includes a conditional branch, which makes it impossible to know for sure which instruction will be executed, the processor makes an educated guess based on the past outcomes of checking the same condition and proceeds to execute the code speculatively. If the prediction proves to be correct, the pipelining can be as good as it was for the unconditional code. If the prediction is wrong, the processor has to discard the result of every instruction that should not have been evaluated, fetch the instructions that it previously assumed wouldn't be needed, and evaluate them instead. This event is called a pipeline flush, and it is an expensive occurrence indeed.

Now we have a better understanding of our previous benchmark in Figure 3.20: both loops have a condition for checking the end of the loop. However, it is predicted almost perfectly. The pipeline flush occurs only once at the end of the loop. The conditional benchmark also has a branch that is based on a random number: if(b1[i]) where b1[i] is true 50\% of the time, randomly. The processor is powerless to predict the outcome, and the pipeline is disrupted half the time (or worse, if we manage to confuse the CPU into actually making wrong predictions).

We should be able to verify our understanding with a direct experiment: all we need is to change the random condition to something that is always true. The only catch is that we have to do it in a way that the compiler cannot figure it out. One common way is to change the initialization of the condition vector as follows:

\begin{lstlisting}[style=styleCXX]
c1[i] = rand() >= 0;
\end{lstlisting}

The compiler doesn't know that the function rand() always returns non-negative random numbers and will not eliminate the condition. The branch predictor circuit of the CPU will quickly learn that the condition if(b1[i]) always evaluates to true and will execute the corresponding code speculatively. We can compare the performance of the well-predicted branch with that of the unpredictable one:

\hspace*{\fill} \\ %插入空行
\begin{center}
\includegraphics[width=0.9\textwidth]{content/1/chapter3/images/22.jpg}\\
Figure 3.22
\end{center}

Here we can see that the cost of the well-predicted branch is minimal and that it is much faster than exactly the same code with a branch that is predicted poorly.

\subsubsubsection{3.5.2\hspace{0.2cm}Profiling for branch mispredictions}

Now that you have seen how badly a single mispredicted branch can impact the performance of the code, you may wonder, how would you ever find such code so you can optimize it? Sure, the function containing this code will take longer than you might expect, but how do you know whether it's because of the badly predicted branches or due to some other inefficiency? By now, you should know enough to avoid making guesses about performance in general; however, speculating about the effectiveness of the branch predictor is particularly futile. Fortunately, most profilers can profile not just execution time but also various factors determining the efficiency, including the branch prediction failures.

In this chapter, we will once again use the perf profiler. As the first step, we can run this profiler to collect the overall performance metrics of the entire benchmark program:

\begin{tcblisting}{commandshell={}}
$ perf stat ./benchmark
\end{tcblisting}

Here are the perf results for the program running only the BM\_branch\_not\_predicted benchmark (other benchmarks are commented out for this test):

\hspace*{\fill} \\ %插入空行
\begin{center}
\includegraphics[width=0.9\textwidth]{content/1/chapter3/images/23.jpg}\\
Figure 3.23 – Profile of a benchmark with a poorly predicted branch
\end{center}

As you can see, 11\% of all branches were mispredicted (the last line of the report). Note that this number is cumulative for all branches, including the perfectly predictable end of the loop condition, so 11\% total is quite bad. We should compare it with our other benchmark, BM\_branch\_predicted, which is identical to this one except the condition is always true:

\hspace*{\fill} \\ %插入空行
\begin{center}
\includegraphics[width=0.9\textwidth]{content/1/chapter3/images/24.jpg}\\
Figure 3.24 – Profile of a benchmark with a well-predicted branch
\end{center}

This time, less than 0.1\% of all branches were not predicted correctly.

The overall performance report is very useful, do not ignore its potential: it can be used to highlight or dismiss some possible causes of poor performance quickly. In our case, we can immediately conclude that our program suffers from one or more mispredicted branches. Now we just need to find which one, and the profiler can help with that as well. Just like in the previous chapter, where we have used the profiler to find out where in the code does our program spends the most time, we can generate a detailed line-by-line profile of branch predictions. We just need to specify the right performance counter to the profiler:

\begin{tcblisting}{commandshell={}}
$ perf record -e branches,branch-misses ./benchmark
\end{tcblisting}

In our case, we can copy the name of the counter from the output of perf stat, because it happens to be one of the counters it measures by default, but the complete list can be obtained by running perf --list.

The profiler runs the program and collects the metrics. We can view them by generating a profile report:

\begin{tcblisting}{commandshell={}}
$ perf report
\end{tcblisting}

The report analyzer is interactive and lets us navigate to the branch mispredictions counter for each function:

\hspace*{\fill} \\ %插入空行
\begin{center}
\includegraphics[width=0.9\textwidth]{content/1/chapter3/images/25.jpg}\\
Figure 3.25 – Detailed profile report for mispredicted branches
\end{center}

Over 99\% of all mispredicted branches occur in just one function. Since the function is small, finding the responsible conditional operation should not be hard. In a larger function, we would have to look at the line-by-line profile.

The branch prediction hardware of a modern processor is fairly sophisticated. For example, if a function is called from two different places and, when called from the first place, a condition usually evaluates to true, while, when called from the second place, the same condition evaluates to false, the predictor will learn that pattern and predict the branch correctly based on the origin of the function call. Similarly, the predictor can detect fairly complex patterns in the data. For example, we can initialize our random condition variables so that the values always alternate, the first one is random, but the next one is the opposite of the first one, and so on:

\begin{lstlisting}[style=styleCXX]
for (size_t i = 0; i < N; ++i) {
	if (i == 0) c1[i] = rand() >= 0;
	else c1[i] = !c1[i - 1];
}
\end{lstlisting}

The profiler confirms that the branch prediction rate for this data is excellent:

\hspace*{\fill} \\ %插入空行
\begin{center}
\includegraphics[width=0.9\textwidth]{content/1/chapter3/images/26.jpg}\\
Figure 3.26 – Branch prediction rate for a "true-false" pattern
\end{center}

We are almost ready to apply our knowledge of how to use the processor efficiently. But first, I must admit that we have overlooked a major potential problem. 































