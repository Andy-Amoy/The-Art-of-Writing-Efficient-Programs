
There are many books and articles that present best practices for API design. They usually focus on usability, clarity, and flexibility. The common guidelines, such as "make the interfaces clear and easy to use correctly" and "make it difficult to misuse the interfaces," do not directly address performance but also do not interfere with the practices that promote good performance and efficiency. In the previous section, we have addressed two important guidelines that should be remembered when designing interfaces for performance. In this section, we will explore some more specific guidelines that target performance explicitly. Many high-performance programs rely on concurrent execution, so it makes sense to address design for concurrency first.

\subsubsubsection{12.4.1\hspace{0.2cm}API design for concurrency}

The most important rule when designing concurrent components and their interfaces is to provide clear thread-safety guarantees. Note that "clear" does not mean "strong": in fact, for optimum performance, it is often better to provide weaker guarantees on low-level interfaces. The approach chosen by the STL is a fine example to follow: all methods that may change the state of the object offer the weak guarantee: the program is well-defined as long as only one thread is using the container at any time. 

If you want a stronger guarantee, you can use locks at the application level. A much better practice is to create your own locking classes that offer a strong guarantee on the interfaces you want. Sometimes, these classes are just locking decorators: they wrap every member function of the decorated object in a lock. More often, there are multiple operations that must be protected by a single lock.

Why? Because it makes no sense to allow the clients to see a particular data structure after "half " of the operation is done. This leads us to a more general observation: as a rule, the thread-safe interfaces should also be transactional. The state of the component (class, server, database, and so on) should be valid before an API call is made and after it is made. All invariants promised by the interface contract should be maintained. It is highly likely that, during the execution of the requested member function (for classes), the object went through one or more states that would not be considered as valid by the clients: it does not maintain the specified invariants. The interface should make it impossible for another thread to observe the object in such an invalid state. Let us illustrate with an example.

Recall our index tree from the previous section. If we want to make this tree thread-safe (which is a short-hand for offering the strong guarantee), we should make inserting new elements safe even when called from multiple threads at the same time:

\begin{lstlisting}[style=styleCXX]
template <typename T> class index_tree {
	public:
	void insert(const T& t) {
		std::lock_guard guard(m_);
		data_.push_back(t);
		idx_.insert(&(data_[data_.size() - 1]));
	}
	private:
	std::set<T*, compare_ptr<T>> idx_;
	std::vector<T> data_;
	std::mutex m_;
};
\end{lstlisting}

Of course, other methods have to be protected as well. It is obvious that we do not want to lock the push\_back() and the insert() calls separately: what would the client do with an object that has the new element in the data store but not in the index? According to our interface, it is not even defined whether or not this new element is in the container: if we scan the index using the iterators, it is not, but if we scan the data store using find(), then it is. This inconsistency tells us that the invariants of the index tree container are maintained before and after but not in the middle of the insertion. Therefore, it is very important that no other thread can see such an ill-defined state. We accomplish this by making sure that the interface is both thread-safe and transactional. It is safe to call multiple member functions concurrently; some threads will block and wait for other threads to complete their work, but there is no undefined behavior. Each member function moves the object from one well-defined state to another well-defined state (in other words, it executes a transaction such as adding a new element). The combination of these two factors makes the object safe to use.

If you need a counter-example (what not to do when designing interfaces for concurrency), recall the discussion of std::queue in Chapter 7, Data Structures for Concurrency. The interface for removing elements from the queue is not transactional: front() returns the front element but does not remove it, while pop() removes the front element but returns nothing, and both yield undefined behavior if the queue is empty. Locking these methods individually does us no good, so a thread-safe API has to use one of the approaches we considered in Chapter 7, Data Structures for Concurrency, to construct a transaction and guard it with a lock.

Now we turn to efficiency: as you can see, it would do us no good if the individual objects that serve as building blocks of our container did their own locking. Imagine if std::deque<T>::push\_back() was itself guarded by a lock. It would make the deque thread-safe (assuming other relevant methods were locked too, of course). But it would not do us any good since we still need to guard the entire transaction with a lock. All it does is wastes some time acquiring and releasing a lock that we do not need.

Also, remember that not all data is being accessed concurrently. In a well-designed program that minimizes the amount of shared state, most work is done on thread-specific data (objects and other data that is exclusive to one thread) and updates to the shared data are relatively infrequent. The objects that are exclusive to one thread should not incur the overhead of locking or other synchronization.

It seems that we now have a contradiction: on the one hand, we should design our classes and other components with thread-safe transactional interfaces. On the other hand, we should not burden these interfaces with locks or other synchronization mechanisms because we might be building higher-level components that do their own locking. 

The general approach to resolving this contradiction is to do both: provide non-locking interfaces that can be used as building blocks of higher-level components and provide thread-safe interfaces where it makes sense. Often, the latter is accomplished by decorating the non-locking interface with a lock guard. Of course, this has to be done within reason. First of all, any non-transactional interfaces are there exclusively for single-threaded use or for building higher-level interfaces. Either way, they do not need to be locked. Second, there are some components and interfaces that, in a particular design, are used in a narrow context. Maybe a data structure is designed specifically for the work that is being done on each thread separately; again, there is no reason to add the overhead of concurrency to it. Some components may be, by design, intended for concurrent use only and are top-level components – they should have thread-safe transactional interfaces. This still leaves many classes and other components that are likely to be used both ways and need locking and non-locking variants. 

There are, fundamentally, two ways to go about it. The first is to design a single component that can use locking if requested, for example:

\begin{lstlisting}[style=styleCXX]
template <typename T> class index_tree {
	public:
	explicit index_tree(bool lock) : lock_(lock) {}
	void insert(const T& t) {
		optional_lock_guard guard(lock_ ? &m_ : nullptr);
		…
	}
	private:
	…
	std::mutex m_;
	const bool lock_;
};
\end{lstlisting}

For this to work, we need a conditional lock\_guard. It is possible to construct one using std::optional or std::unique\_ptr, but it's inelegant and inefficient. It is much easier to write our own RAII class similar to std::lock\_guard:

\begin{lstlisting}[style=styleCXX]
template <typename L> class optional_lock_guard {
	L* lock_;
	public:
	explicit optional_lock_guard(L* lock) : lock_(lock) {
		if (lock_) lock_->lock();
	}
	~optional_lock_guard() {
		if (lock_) lock_->unlock();
	}
	optional_lock_guard(const optional_lock_guard&) = delete;
	// Handle other copy/move operations.
};
\end{lstlisting}

In addition to being non-copyable, std::lock\_guard is also non-movable. You can follow the same design or make your class movable. For classes, you can often handle the locking condition at compile time instead of runtime. This approach uses a policy-based design with a locking policy:

\begin{lstlisting}[style=styleCXX]
template <typename T, typename LP> class index_tree : private 
LP {
	public:
	void insert(const T& t) {
		std::lock_guard<LP> guard(*this);
		…
	}
};
\end{lstlisting}

We should have at least two versions of the locking policy LP:

\begin{lstlisting}[style=styleCXX]
struct locking_policy {
	std::mutex m_;
	void lock() { m_.lock(); }
	void unlock() { m_.unlock(); }
};
struct non_locking_policy {
	void lock() {}
	void unlock() {}
};
\end{lstlisting}

Now we can create index\_tree objects with weak or strong thread-safety guarantees:

\begin{lstlisting}[style=styleCXX]
index_tree<int, locking_policy> strong_ts_tree;
index_tree<int, non_locking_policy> weak_ts_tree;
\end{lstlisting}

Of course, this compile-time approach works well for classes but may not be applicable to other types of components and interfaces. For example, when communicating with a remote server, you may want to notify it at runtime whether the current session is shared or exclusive.

The second option is the one we discussed earlier, a locking decorator. In this version, the original class (index\_tree) offers only the weak thread-safety guarantee. The strong guarantee is provided by this wrapper class:

\begin{lstlisting}[style=styleCXX]
template <typename T> class index_tree_ts :
private index_tree<T> 
{
	public:
	using index_tree<T>::index_tree;
	void insert(const T& t) {
		std::lock_guard guard(m_);
		index_tree<T>::insert(t);
	}
	private:
	std::mutex m_;
};
\end{lstlisting}

Note that, while encapsulation is generally preferred to inheritance, the advantage of the inheritance here is that we can avoid copying all the constructors of the decorated class. 

The same approaches can be applied to other APIs: an explicit parameter to control locking vs. a decorator. Which one to use depends largely on the particulars of your design – they both have their pros and cons. Note that, even if the overhead of locking is insignificant compared to the work done by a particular API call, there may be good reasons to avoid gratuitous locking: in particular, such locking greatly increases the amount of code that should be vetted for possible deadlocks.

Note that there is a lot of overlap between the guideline that all thread-safe interfaces should be transactional and the best practices for designing exception-safe, or, more generally, error-safe interfaces. The latter is more complex because not only do we have to guarantee a valid state before and after the call to an interface but also that the system remains in a well-defined state after an error is detected. 

From the point of view of performance, error handling is essentially overhead: we do not expect errors to be frequent (otherwise, they are not really errors but regularly occurring situations we have to deal with). Fortunately, the best practices for writing error-safe code, such as using RAII objects for cleanup, are also quite efficient and rarely impose significant overhead. Nonetheless, some error conditions are quite difficult to detect reliably, as we have seen in Chapter 11, Undefined Behavior and Performance.

There are several guidelines for designing efficient concurrent APIs that we have learned in this section:

\begin{itemize}
\item 
Interfaces intended for concurrent use should be transactional.

\item 
Interfaces should provide the minimum necessary thread-safety guarantee (weak guarantee for interfaces that are not intended to be used concurrently).

\item 
For interfaces that are used both as a client-visible API and as building blocks for higher-level components that create their own, more complex transactions and provide the appropriate locking, it is often desirable to have two versions: one with the strong thread-safety guarantee and another with the weak one (or, locking and non-locking). This can be done with conditional locking or using decorators.
\end{itemize}

These guidelines are in general agreement with other best practices for designing robust and clear APIs. Thus, it is rare that we have to make design trade-offs to allow better performance. 

Let us now leave behind the issues of concurrency and turn to other areas of design for performance.

\subsubsubsection{12.4.2\hspace{0.2cm}Copying and sending data}

This discussion is going to be a generalization of the matters we covered in Chapter 9, High-Performance C++, when we talked about unnecessary copying. Using any interface, not just a C++ function call, usually involves sending or receiving some data. This is a very general notion, and we won't be able to offer any specific guidelines that are universally applicable beyond the equally general "be mindful of the cost of data transfer." We can elaborate this a little for some common types of interfaces. 

We have already discussed the overhead of copying memory in C++ and the resulting considerations for the interfaces. We covered the implementation techniques in Chapter 9, High-Performance C++. For the design, we can emphasize the generally important guideline: have a well-defined data ownership and lifetime management. The reason it comes up in the context of performance is that often excessive copying is a side effect of muddled ownership, a workaround for data disappearing while it's still being used because the lifetime of many pieces of the complex system is not well-understood. 

A very different set of issues needs to be managed in distributed programs, client-server applications, or, generally, any interface between components where bandwidth constraints matter. In these situations, data compression is often used: we trade CPU time for bandwidth because it costs processing time to compress and decompress the data, but the transmission is going to be faster. Often, the decision of whether to compress the data in a particular channel cannot be made at the design time: we simply don't know enough to make an informed trade-off. Thus, it is important to design the system to allow for the possibility of compression. This has some non-trivial implications for designing the interfaces of the data structures that may be converted to a compressed format. If your design calls for compressing the entire set of data, transmitting it, then converting it back to the decompressed format, then the interfaces you use to work with the data do not change, but the memory requirements grow because you will have both compressed and uncompressed representations stored in memory at some point. The alternative is a data structure that stores compressed data internally, which takes some forethought when it comes to designing its interfaces. 

As an example, imagine that we have a simple struct for storing three-dimensional locations and maybe some attributes:

\begin{lstlisting}[style=styleCXX]
struct point {
	double x, y, z;
	int color;
	… maybe more data …
};
\end{lstlisting}

A very popular guideline says that we should avoid getter and setter methods that do nothing but access the corresponding data member; we are advised against doing this:

\begin{lstlisting}[style=styleCXX]
class point {
	double x, y, z;
	int color;
	public:
	double get_x() const { return x; }
	void set_x(double x_in) { x = x_in; } // Same for y etc
};
\end{lstlisting}

We store these objects in a collection of points:

\begin{lstlisting}[style=styleCXX]
class point_collection {
	point& operator[](size_t i);
};
\end{lstlisting}

This design served us fine for a while, but the requirements evolved, and now we have to store and transmit millions of points. It is hard to imagine how we might introduce internal compression with this interface: the index operator returns a reference to an object that must have three double data members accessible directly. If we had getters and setters, we might have been able to implement the point as a proxy to a compressed set of points inside the collection:

\begin{lstlisting}[style=styleCXX]
class point {
	point_collection& coll_;
	size_t point_id_;
	public:
	double get_x() const { return coll_[point_id_]; }
	…
};
\end{lstlisting}

The collection stores compressed data and can decompress parts of it on the fly to get access to the point identified by the point\_id\_. 

Of course, an even more compression-friendly interface would be one that requires us to iterate over the entire collection of points sequentially. Now you should realize that we just revisited the guideline that instructs us to reveal as little information as possible about the internal workings of our collection. The focus on compression serves to provide us with a particular point of view. If you think about the possibility of data compression, or, generally, alternative data representations for storage and transmission, you have to also think about restricting access to this data. Maybe you can come up with algorithms that do all the required computations without using random access to the data? If you limit access by design, you preserve the possibility of compressing the data (or taking advantage of the limited access pattern in some other way). 

There are other types of interfaces, of course, and they all have their own runtime, memory, and storage space costs associated with transmitting large volumes of data. When designing for performance, consider the possibility that these costs will become performance-critical and try to limit the interfaces for maximum freedom of internal data representation. Of course, this, like anything else, should be practiced within reason; it is highly unlikely that a hand-written configuration file will ever become a performance bottleneck (computers read faster than you write, in any format). 

We have touched on the matter of data layout as it affects the interface design. Let us now focus directly on the performance impact of data organization.


























