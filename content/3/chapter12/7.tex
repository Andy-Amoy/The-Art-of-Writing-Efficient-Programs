It is not only when making decisions about trade-offs that we have to stand of the firm foundation of good performance data. After all, how can we make decisions about designing data structures for efficient memory access if we do not know how much it costs to access data in a cache-optimal order as opposed to some random order? This comes back to the first rule of performance, which you should have memorized by now: never guess about performance. This is easier said than done if our program exists as a scattering of design diagrams on a whiteboard. 

You can't run a design, so how do you get measurements to guide and back up your design decisions? Some of the knowledge comes with experience. By this, I don't mean the kind of experience that says, "we have always done it this way." But you may have designed and implemented similar components and other parts of the new system. If they are reusable, they come with reliable performance information. But even if you have to modify them or design something similar, you have highly relevant performance measurements that likely transfer well to the new design. 

What should we do, then, if we have no relevant programs that can be used to measure performance? This is when we have to fall back on models and prototypes. Models are artificial constructs that mimic the expected workload and performance of some parts of our future program, to the best of our knowledge. For example, if we have to make a decision about organizing large amounts of data in memory and we know that we will have to frequently process the entire data corpus, our micro-benchmarks from Chapter 4, Memory Architecture and Performance, are the kind of model you might use: process the same volume of data organized as a list vs. an array. This is a model, not an exact measurement of your future program's performance, but it provides valuable insight and gives you good data to support your decisions. Just remember that the more approximate the model is, the more inaccurate the predictions are: if you model two alternative designs and come up with performance measurements within 10\% of each other, you should probably consider it a wash. By the way, this does not make it a waste: you obtained important information, both design options offer similar performance, so you are free to choose based on other criteria. 

Not all models are micro-benchmarks. Sometimes you can use existing programs to model new behavior. Say you have a distributed program that operates on some data similar to what your next program needs to deal with. The new program will have much more data, and the similarity is only superficial (maybe both programs work on strings), so the old program cannot be used to do any real measurements of handling the new data. No matter: we can modify the code to send and receive much longer strings. What if our existing program makes no use of them? That's ok, too: we will write some code to generate and consume these strings in a somewhat realistic manner and embed it in the program. Now we can fire up the part of the program that does the distributed computations and see how long it takes to send and receive the expected volumes of data. Let's assume it takes long enough that we are considering compression. We can do better than that, though: add compression to the code and compare network transfer speedup with compression and decompression costs. If you don't want to invest a lot of time writing a realistic compression algorithm for your specific data, try reusing an existing compression library. Comparing several compression algorithms from freely available libraries will give you even more valuable data for a later time when you have to decide how much compression is optimal. 

Note carefully what we have just done: we used an existing program as a framework to run some new code that approximates the behavior of the future program. In other words, we have constructed a prototype. Prototyping is another way to get performance estimates for making design decisions. Of course, building prototypes for performance is somewhat different from making feature-based prototypes. In the latter case, we want to quickly put together a system that demonstrates the desired behavior, usually with no regard for the performance or quality of the implementation. A performance prototype should give us reasonable performance numbers, so the low-level implementation must be efficient. We can neglect corner cases and error handling. We can also skip many features as long as the ones we prototype do exercise the code we want to benchmark. Sometimes, our prototype will have no features at all: instead, somewhere in the code, we will hard-code a condition that in a real system happens when certain features are exercised. The high-performance code we have to create during such prototyping often forms the foundation of our low-level libraries later.

It should be pointed out that all models are approximate, and they would still be approximate even if you had a complete and final implementation for the code whose performance you are trying to measure. The micro-benchmarks are, generally, less accurate than larger frameworks, which gives rise to catchy titles like "micro-benchmarks are lies." The main reason the micro-benchmarks and other performance models do not always match the eventual results is that any program's performance is affected by its environment. For example, you may benchmark a piece of code for optimal memory access, only to find out that it's usually running alongside other threads that completely saturate the memory bus. 

Just like it's important to understand the limitations of the models, it is also important to not over-react. Benchmarks do provide useful information. The more complete and realistic the measured software is, the more accurate the results are. If the benchmark shows one piece of code several times faster than the other, this difference is unlikely to disappear completely once the code is running in its final context. But it would be a folly to try to get the last 5\% of efficiency from anything other than the final version of the code running on the real data.

The prototypes – approximations for the real programs that reproduce with some degree of accuracy the properties we are interested in – allow us to get reasonable estimates of performance that would follow from different design decisions. They can range from micro-benchmarks to experiments on large, preexisting programs, but they all serve one goal: move design for performance from the realm of guesswork to the foundation of sound measurement-driven decisions. 
